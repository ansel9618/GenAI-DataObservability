# Quick helper

# log into clickhouse container and run client
docker compose exec clickhouse clickhouse-client

# drop table
DROP TABLE IF EXISTS logs;

# recreate log table
CREATE TABLE logs (
    timestamp DateTime,
    level String,
    service String,
    job_name String,
    step_name String,
    pipeline_id String,
    duration_ms UInt32,
    records_processed UInt32,
    records_failed UInt32,
    status_code UInt16,
    host String,
    thread String,
    environment String,
    tags Array(String),
    message String,
    request_id String
) ENGINE = MergeTree()
ORDER BY timestamp;

# check table structure and data
DESCRIBE TABLE logs;
DESCRIBE TABLE logs;

# Docker path variable
export PATH="$PATH:/Applications/Docker.app/Contents/Resources/bin"


## âš ï¸ Is WAL a Problem Under High Load?

Yes â€” it's important to think ahead if you're expecting a high volume of logs.

### ðŸ” What's happening now?

Each request to FastAPI:
1. Encodes a vector with `SentenceTransformer`
2. Writes the full JSON log to `wal.jsonl` (on disk)
3. Sends the embedding + payload to Qdrant

This means every request touches the file system â€” even for a single log â€” which can cause:

| Risk                             | Explanation |
|----------------------------------|-------------|
| âš ï¸ **Disk I/O pressure**         | Appending to a file thousands of times per second can slow things down |
| ðŸ” **File lock contention**      | Pythonâ€™s file writes are synchronous and blocking â€” no lock but still blocking |
| ðŸ§± **CPU bottleneck** (embedding) | SentenceTransformer is not async or GPU-accelerated unless you configure it |
| ðŸ§¨ **Qdrant overload**           | Too many writes to Qdrant could slow vector indexing |

---

### âœ… You're safe for...

- ðŸ’¡ Dev / low-traffic apps
- ðŸ“Š Infrequent analytics use cases
- ðŸ§ª Prototyping and testing

---

### ðŸ§  But for **high-throughput logging**, consider this:

#### âœ… Better write performance with **buffering**:
Use an in-memory buffer (e.g. `queue.Queue`) and flush to WAL in batches (e.g. every 1 sec or 50 logs). Faster and reduces disk writes.

#### âœ… Use **async file I/O**:
Switch from `open(..., "a")` to `aiofiles` or a dedicated log buffer thread to avoid blocking FastAPI handlers.

#### âœ… Run SentenceTransformer in **background thread or worker**:
It's CPU-heavy. Either offload to a background task (like Celery) or batch embeddings.

#### âœ… Use a real message queue (optional):
If youâ€™re really scaling, tools like **Kafka**, **Redis Streams**, or **RabbitMQ** can replace the WAL, giving you full decoupling and resilience.

---

### âœ… Summary Table

| Load Level           | Action Needed       |
|----------------------|---------------------|
| ðŸ’¡ Low (dev/test)     | âœ… You're fine       |
| âš ï¸ Medium (burst logs)| ðŸ”„ Add buffer to WAL |
| ðŸ”¥ High throughput    | ðŸ§° Consider queue, async I/O, batching |


                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FastAPI    â”‚
                    â”‚             â”‚
                    â”‚ Writes to   â”‚
                    â”‚ wal.jsonl   â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚ WAL Processor       â”‚
             â”‚ (inserts into       â”‚
             â”‚  logs.duckdb)       â”‚
             â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ logs.duckdb     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–²
                  â”‚  (read-only)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Streamlit        â”‚
        â”‚ Loads into memory ðŸ§  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


--> WAL could be removed because not really necessary right now